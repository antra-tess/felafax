trainer_dir: "/mnt/persistent-disk/test/"
export_dir: "/mnt/persistent-disk/test/hf_export"
hf_token: ""
hf_repo: "felarof01/test-model"
test_mode: true

data_config:
  batch_size: 16
  max_seq_length: 2048

trainer_config:
  model_name: "meta-llama/Llama-3.2-1B-Instruct"
  param_dtype: "bfloat16"
  compute_dtype: "bfloat16"
  num_epochs: 1
  num_steps: 5
  mesh_shape: [1, 2, 4]  # Use list format for tuple
  learning_rate: 1e-3
  lora_rank: 16
  use_lora: true
  log_interval: 5
  eval_interval: 5
  eval_steps: 10


checkpointer_config:
  save_interval_steps: 100