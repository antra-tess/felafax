hf_token: "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"
base_dir: "/mnt/persistent-disk/test/"
test_mode: true

data:
  batch_size: 16
  max_seq_length: 2048

trainer:
  model_name: "meta-llama/Llama-3.2-1B-Instruct"
  param_dtype: "bfloat16"
  compute_dtype: "bfloat16"
  num_epochs: 1
  num_steps: 5
  mesh_shape: [1, 2, 4]  # Use list format for tuple
  learning_rate: 1e-3
  lora_rank: 16
  use_lora: false
  log_interval: 5
  eval_interval: 5
  eval_steps: 10


checkpoint:
  save_interval_steps: 100